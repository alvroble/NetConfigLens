{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "37YhuBA1_RRm"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZWBNIwtBeLL2ETCW/skc+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvroble/annec/blob/main/Prueba_CustomTokenizer%2BGPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOcR9c5TGTfP",
        "outputId": "0106b860-687b-436b-e5c6-94db345bf380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dngd8QORFTJ7"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/SANDS')\n",
        "path_files ='/content/drive/MyDrive/Colab Notebooks/UC10'\n",
        "path_base = '/content/drive/MyDrive/Colab Notebooks'\n",
        "path_config_files = '/content/drive/MyDrive/Colab Notebooks/UC10/configs'\n",
        "path_dataset = '/content/drive/MyDrive/Colab Notebooks/UC10/dataset'\n",
        "path_model='/content/drive/MyDrive/Colab Notebooks/TFM_Transformers/GPT2_CustomTokenizer_Model'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom tokenizer"
      ],
      "metadata": {
        "id": "9caCwbGS_JXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizer\n",
        "!pip install gensim\n",
        "!pip install datasets\n"
      ],
      "metadata": {
        "id": "Zhlp9GLU_JCt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e334965d-1275-49bb-bf21-601120a171c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Collecting tokenizer\n",
            "  Downloading tokenizer-3.4.3-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizer\n",
            "Successfully installed tokenizer-3.4.3\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, GPT2Config, TFGPT2LMHeadModel\n",
        "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
        "from pathlib import Path\n",
        "import os\n",
        "from gensim.corpora import WikiCorpus\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFD, Sequence\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "from datasets import Dataset\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Vt2zm5wT_hFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = Sequence([NFD()])\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "tokenizer.decoder = ByteLevelDecoder()"
      ],
      "metadata": {
        "id": "ipDVUM77ABB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = BpeTrainer(vocab_size=50000, show_progress=True, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\"\n",
        "])\n"
      ],
      "metadata": {
        "id": "sx44I6XGAJeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_files = [f\"{path_config_files}/{f}\" for f in listdir(path_config_files) if (isfile(join(path_config_files, f)))]"
      ],
      "metadata": {
        "id": "dJY_0iLrAY4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(files=config_files, trainer=trainer)"
      ],
      "metadata": {
        "id": "JHz71usjAKjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = './tokenized_data'\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "tokenizer.model.save(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxyY3TClA46R",
        "outputId": "f632feb4-9bf1-4501-f91b-c28ff35e9a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./tokenized_data/vocab.json', './tokenized_data/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(save_path, unk_token=\"[UNK]\")"
      ],
      "metadata": {
        "id": "godvLtMyBV7k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "4d3af1ab-94a8-4042-b946-40bcf6f23aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5646e3b4e826>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"[UNK]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'save_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.add_special_tokens({\n",
        "    \"eos_token\": \"</s>\",\n",
        "    \"bos_token\": \"<s>\",\n",
        "    \"unk_token\": \"<unk>\",\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"mask_token\": \"<mask>\"\n",
        "})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zh8FfbvCAD8",
        "outputId": "2ad2bde2-1687-409a-94f2-84f01d1d3219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset"
      ],
      "metadata": {
        "id": "37YhuBA1_RRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(join(path_dataset,\"dataset-configs-cisco.jsonl\"),orient=\"records\",lines=True)\n",
        "dataset = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "pyIJMuWcVNgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"config\"][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1_Bvh9pQjFzR",
        "outputId": "cfa343ba-2e02-46ab-8d18-80585bd2250e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!Image: Software version: IOS-XR 7.6R3\\n!Image: Copyright (C) 2020 your favorite vendor. All rights reserved\\n!Image: Software Product: IOS-XR, Version: 7.6R3\\n!Image: Hardware Model: ASR\\n!Image: Software Feature Code: SP\\n!Image: System Configuration Code: S3\\n!Image: Package Configuration Code: P4\\n!Image: Software Baseline Version: 2.3.5\\n!Image: Installation Information:\\n!Image: Image Filename: IOS-XR-7-6-R-3\\n!Image: ONIE SysInfo: x86_64-accton_asr7736_64x-r0\\n!\\n!\\n!\\n!\\n!\\n!\\n! Software version: IOS-XR 7.6R3\\n!\\n!\\nlogging level cml 5\\n!\\nservice password-encryption\\n!\\nenable confirmation-dialog\\n!\\nno logging console\\nno logging monitor\\nlogging level nsm 5 \\nlogging level rip 5 \\nlogging level ripng 5 \\nlogging level ospf 5 \\nlogging level ospf6 5 \\nlogging level isis 5 \\nlogging level hostp 5 \\nlogging level mrib 5 \\nlogging level pim 5 \\nlogging level auth 5 \\nlogging level mstp 5 \\nlogging level onm 5 \\nlogging level hsl 5 \\nlogging level oam 5 \\nlogging level vlog 5 \\nlogging level trill 5 \\nlogging level vrrp 5 \\nlogging level ndd 5 \\nlogging level rib 5 \\nlogging level bgp 5 \\nlogging level l2mrib 5 \\nlogging level lag 5 \\nlogging level sflow 5 \\nlogging level pserv 5 \\nlogging level cmm 5 \\nbanner motd *** Configuration should not be changed manually ***\\n!\\nip vrf management\\n!\\nbfd interval 3 minrx 3 multiplier 3\\n!\\nload-balance prof1\\nload-balance prof1 macro-flow\\nload-balance prof1 ipv4 dest-ipv4 src-ipv4 destl4-port srcl4-port protocol-id\\nload-balance prof1 ipv6 dest-ipv6 src-ipv6 destl4-port srcl4-port next-hdr\\nload-balance prof1 vxlan inner-l3 dest-ip src-ip destl4-port srcl4-port protocol-id\\nforwarding profile l2-profile-three\\n!\\nqos enable\\nqos statistics\\n!\\nhostname PE-2\\nno ip domain-lookup\\nip domain-lookup vrf management\\nip domain-name vrf management mlfornetworkengineers.ai\\nip name-server vrf management 8.8.8.8\\nip name-server vrf management 8.8.8.9\\nerrdisable cause link-flap\\nno errdisable cause stp-bpdu-guard\\nerrdisable link-flap-setting max-flaps 5 time 60\\nno feature telnet vrf management\\nfeature ssh vrf management\\nfeature tacacs+ vrf management\\ntacacs-server login host 10.15.1.10 vrf management seq-num 1 key 7 0x960c890aa72610d6aca0 port 1623\\ntacacs-server login host 10.15.1.11 vrf management seq-num 2 key 7 0x960c890a272610d6aca0 port 1623\\naaa local authentication unlock-timeout 1\\naaa authentication login default vrf management group tacacs+ local\\nsnmp-server enable snmp vrf management \\nsnmp-server view all .1 included vrf management\\nsnmp-server community test group network-operator vrf management\\nsnmp-server host 10.15.1.12 traps version 2c test udp-port 162 vrf management\\nsnmp-server host 10.15.1.13 traps version 2c test udp-port 162 vrf management\\nfeature ntp vrf management\\nntp enable vrf management\\nntp server 10.15.1.14 vrf management\\nusername admin role network-admin password encrypted $1hde642ysfh5eyw3\\nno username ios\\nfeature rsyslog vrf management\\nlogging server 10.15.1.15 5 facility local0 vrf management\\n!\\n\\ninterface et-1-1-1\\n description core:400G:PE-2-et-1-1-1 C-1 et-1-1-2\\n load-interval 30\\n ip address 11.1.2.2/30\\n mtu 9000\\n port-channel load-balance prof1\\n port-channel min-links 2\\n ip ospf network point-to-point\\n ip ospf cost 10000\\n!\\ninterface et-1-1-2\\n description core:400G:PE-2-et-1-1-2 C-2 et-1-1-2\\n load-interval 30\\n ip address 11.2.2.2/30\\n mtu 9000\\n port-channel load-balance prof1\\n port-channel min-links 2\\n ip ospf network point-to-point\\n ip ospf cost 10000\\n!\\ninterface et-1-1-3\\n description core:400G:PE-2-et-1-1-3 C-3 et-1-1-2\\n load-interval 30\\n ip address 11.3.2.2/30\\n mtu 9000\\n port-channel load-balance prof1\\n port-channel min-links 2\\n ip ospf network point-to-point\\n ip ospf cost 10000\\n!\\ninterface et-1-1-4\\n description core:400G:PE-2-et-1-1-4 C-4 et-1-1-2\\n load-interval 30\\n ip address 11.4.2.2/30\\n mtu 9000\\n port-channel load-balance prof1\\n port-channel min-links 2\\n ip ospf network point-to-point\\n ip ospf cost 10000\\n!\\n\\ninterface et-1-2-1\\n description edge:10G:PE-2-et-1-2-1 E-2-1 et-1-1-1\\n load-interval 30\\n ip address 12.2.1.1/30\\n mtu 1500\\n port-channel load-balance prof1\\n port-channel min-links 2\\n ip ospf network point-to-point\\n ip ospf cost 1000\\n!\\ninterface et-1-2-2\\n description edge:10G:PE-2-et-1-2-2 E-2-2 et-1-1-1\\n load-interval 30\\n ip address 12.2.2.1/30\\n mtu 1500\\n port-channel load-balance prof1\\n port-channel min-links 2\\n ip ospf network point-to-point\\n ip ospf cost 1000\\n!\\n\\n\\ninterface eth0\\n ip vrf forwarding management\\n ip address 10.2.2.2\\n!\\ninterface lo\\n ip address 127.0.0.1/8\\n ip address 10.2.2.2 secondary\\n ipv6 address ::1/128\\n ip ospf cost 1\\n!\\ninterface lo.management\\n ip vrf forwarding management\\n ip address 127.0.0.1/8\\n ipv6 address ::1/128\\n mtu 1500\\n!\\nrouter ospf\\n ospf router-id 10.2.2.2\\n bfd all-interfaces\\n timers spf exp 50 50\\n timers throttle lsa all 0 1 1\\n timers lsa arrival 1\\n passive-interface lo\\n ospf point-point rfc-incompatible\\n network 10.0.1.10/32 area 0.0.0.0\\n network 10.2.1.0/30 area 0.0.0.0\\n network 10.2.2.0/30 area 0.0.0.0\\n network 10.2.3.0/30 area 0.0.0.0\\n network 10.2.3.0/30 area 0.0.0.0\\n network 10.2.4.0/30 area 0.0.0.0\\n!\\nip route vrf management 0.0.0.0/0 10.0.1.10 eth0\\n!\\nline con 0\\n exec-timeout 240 0\\nline vty 0 39\\n exec-timeout 240 0\\n!\\nend\\n!\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.train_test_split(test_size=0.1)"
      ],
      "metadata": {
        "id": "YEiZICIojq6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q05zmH19W4re",
        "outputId": "0de1127f-e3db-40ec-b423-67394e172393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'device_name': 'PE-93',\n",
              " 'config': '!Image: Software version: IOS-XR 7.6R3\\n!Image: Copyright (C) 2020 your favorite vendor. All rights reserved\\n!Image: Software Product: IOS-XR, Version: 7.6R3\\n!Image: Hardware Model: ASR\\n!Image: Software Feature Code: SP\\n!Image: System Configuration Code: S3\\n!Image: Package Configuration Code: P4\\n!Image: Software Baseline Version: 2.3.5\\n!Image: Installation Information:\\n!Image: Image Filename: IOS-XR-7-6-R-3\\n!Image: ONIE SysInfo: x86_64-accton_asr7736_64x-r0\\n!\\n!\\n!\\n!\\n!\\n!\\n! Software version: IOS-XR 7.6R3\\n!\\n!\\nlogging level cml 5\\n!\\nservice password-encryption\\n!\\nenable confirmation-dialog\\n!\\nno logging console\\nno logging monitor\\nlogging level nsm 5 \\nlogging level rip 5 \\nlogging level ripng 5 \\nlogging level ospf 5 \\nlogging level ospf6 5 \\nlogging level isis 5 \\nlogging level hostp 5 \\nlogging level mrib 5 \\nlogging level pim 5 \\nlogging level auth 5 \\nlogging level mstp 5 \\nlogging level onm 5 \\nlogging level hsl 5 \\nlogging level oam 5 \\nlogging level vlog 5 \\nlogging level trill 5 \\nlogging level vrrp 5 \\nlogging level ndd 5 \\nlogging level rib 5 \\nlogging level bgp 5 \\nlogging level l2mrib 5 \\nlogging level lag 5 \\nlogging level sflow 5 \\nlogging level pserv 5 \\nlogging level cmm 5 \\nbanner motd *** Configuration should not be changed manually ***\\n!\\nip vrf management\\n!\\nbfd interval 3 minrx 3 multiplier 3\\n!\\nload-balance prof1\\nload-balance prof1 macro-flow\\nload-balance prof1 ipv4 dest-ipv4 src-ipv4 destl4-port srcl4-port protocol-id\\nload-balance prof1 ipv6 dest-ipv6 src-ipv6 destl4-port srcl4-port next-hdr\\nload-balance prof1 vxlan inner-l3 dest-ip src-ip destl4-port srcl4-port protocol-id\\nforwarding profile l2-profile-three\\n!\\nqos enable\\nqos statistics\\n!\\nhostname PE-93\\nno ip domain-lookup\\nip domain-lookup vrf management\\nip domain-name vrf management mlfornetworkengineers.ai\\nip name-server vrf management 8.8.8.8\\nip name-server vrf management 8.8.8.9\\nerrdisable cause link-flap\\nno errdisable cause stp-bpdu-guard\\nerrdisable link-flap-setting max-flaps 5 time 60\\nno feature telnet vrf management\\nfeature ssh vrf management\\nfeature tacacs+ vrf management\\ntacacs-server login host 10.15.1.10 vrf management seq-num 1 key 7 0x960c890aa72610d6aca0 port 1623\\ntacacs-server login host 10.15.1.11 vrf management seq-num 2 key 7 0x960c890a272610d6aca0 port 1623\\naaa local authentication unlock-timeout 1\\naaa authentication login default vrf management group tacacs+ local\\nsnmp-server enable snmp vrf management \\nsnmp-server view all .1 included vrf management\\nsnmp-server community test group network-operator vrf management\\nsnmp-server host 10.15.1.12 traps version 2c test udp-port 162 vrf management\\nsnmp-server host 10.15.1.13 traps version 2c test udp-port 162 vrf management\\nfeature ntp vrf management\\nntp enable vrf management\\nntp server 10.15.1.14 vrf management\\nusername admin role network-admin password encrypted $1hde642ysfh5eyw3\\nno username ios\\nfeature rsyslog vrf management\\nlogging server 10.15.1.15 5 facility local0 vrf management\\n!\\n\\ninterface et-1-1-1\\n description core:400G:PE-93-et-1-1-1 C-1 et-1-1-93\\n load-interval 30\\n ip address 11.1.93.2/30\\n mtu 9000\\n port-channel load-balance prof1\\n port-channel min-links 2\\n ip ospf network point-to-point\\n ip ospf cost 10000\\n!\\ninterface et-1-1-2\\n description core:400G:PE-93-et-1-1-2 C-2 et-1-1-93\\n load-interval 30\\n ip address 11.2.93.2/30\\n mtu 9000\\n port-channel load-balance prof1\\n port-channel min-links 2\\n ip ospf network point-to-point\\n ip ospf cost 10000\\n!\\ninterface et-1-1-3\\n description core:400G:PE-93-et-1-1-3 C-3 et-1-1-93\\n load-interval 30\\n ip address 11.3.93.2/30\\n mtu 9000\\n port-channel load-balance prof1\\n port-channel min-links 2\\n ip ospf network point-to-point\\n ip ospf cost 10000\\n!\\ninterface et-1-1-4\\n description core:400G:PE-93-et-1-1-4 C-4 et-1-1-93\\n load-interval 30\\n ip address 11.4.93.2/30\\n mtu 9000\\n port-channel load-balance prof1\\n port-channel min-links 2\\n ip ospf network point-to-point\\n ip ospf cost 10000\\n!\\n\\ninterface et-1-2-1\\n description edge:10G:PE-93-et-1-2-1 E-93-1 et-1-1-1\\n load-interval 30\\n ip address 12.93.1.1/30\\n mtu 1500\\n port-channel load-balance prof1\\n port-channel min-links 2\\n ip ospf network point-to-point\\n ip ospf cost 1000\\n!\\ninterface et-1-2-2\\n description edge:10G:PE-93-et-1-2-2 E-93-2 et-1-1-1\\n load-interval 30\\n ip address 12.93.2.1/30\\n mtu 1500\\n port-channel load-balance prof1\\n port-channel min-links 2\\n ip ospf network point-to-point\\n ip ospf cost 1000\\n!\\n\\n\\ninterface eth0\\n ip vrf forwarding management\\n ip address 10.2.2.93\\n!\\ninterface lo\\n ip address 127.0.0.1/8\\n ip address 10.2.2.93 secondary\\n ipv6 address ::1/128\\n ip ospf cost 1\\n!\\ninterface lo.management\\n ip vrf forwarding management\\n ip address 127.0.0.1/8\\n ipv6 address ::1/128\\n mtu 1500\\n!\\nrouter ospf\\n ospf router-id 10.2.2.93\\n bfd all-interfaces\\n timers spf exp 50 50\\n timers throttle lsa all 0 1 1\\n timers lsa arrival 1\\n passive-interface lo\\n ospf point-point rfc-incompatible\\n network 10.0.1.10/32 area 0.0.0.0\\n network 10.2.1.0/30 area 0.0.0.0\\n network 10.2.2.0/30 area 0.0.0.0\\n network 10.2.3.0/30 area 0.0.0.0\\n network 10.2.3.0/30 area 0.0.0.0\\n network 10.2.4.0/30 area 0.0.0.0\\n!\\nip route vrf management 0.0.0.0/0 10.0.1.10 eth0\\n!\\nline con 0\\n exec-timeout 240 0\\nline vty 0 39\\n exec-timeout 240 0\\n!\\nend\\n!\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "LPYi9oQUHsPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "gNgHa-lJHqv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFGPT2LMHeadModel(config)"
      ],
      "metadata": {
        "id": "59c51ZVMH4LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_files = [f\"{path_config_files}/{f}\" for f in listdir(path_config_files) if (isfile(join(path_config_files, f)))]\n",
        "single_string = ''\n",
        "for filename in config_files:\n",
        "    with open(filename, \"r\", encoding='utf-8') as f:\n",
        "        x = f.read()\n",
        "    single_string += x + tokenizer.eos_token\n",
        "\n",
        "# tokenize dataset\n",
        "string_tokenized = tokenizer.encode(single_string)\n",
        "print(\"Done tokenizing\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL2Scr5hH6ad",
        "outputId": "8c3705f5-6c88-4489-b511-4f79bdc12e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done tokenizing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = []\n",
        "block_size = 100\n",
        "BATCH_SIZE = 12\n",
        "BUFFER_SIZE = 1000\n",
        "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
        "    examples.append(string_tokenized[i:i + block_size])\n",
        "inputs, labels = [], []\n",
        "\n",
        "for ex in examples:\n",
        "    inputs.append(ex[:-1])\n",
        "    labels.append(ex[1:])\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(\"Done creating dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaQwjnfiIYbv",
        "outputId": "16dfca9d-b68f-4390-909e-ad45457b89af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done creating dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=[\n",
        "              loss, *[None] * model.config.n_layer], metrics=[metric])"
      ],
      "metadata": {
        "id": "PiWSFv15Ihdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 10\n",
        "history = model.fit(dataset, epochs=num_epoch, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNf5XTydIlLA",
        "outputId": "f1f351aa-485d-4058-9552-64a2aff922de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "149/149 [==============================] - 100s 278ms/step - loss: 2.9453 - accuracy: 0.4470\n",
            "Epoch 2/10\n",
            "149/149 [==============================] - 44s 296ms/step - loss: 1.0942 - accuracy: 0.7552\n",
            "Epoch 3/10\n",
            "149/149 [==============================] - 42s 282ms/step - loss: 0.6238 - accuracy: 0.8486\n",
            "Epoch 4/10\n",
            "149/149 [==============================] - 42s 284ms/step - loss: 0.4380 - accuracy: 0.8919\n",
            "Epoch 5/10\n",
            "149/149 [==============================] - 42s 282ms/step - loss: 0.3304 - accuracy: 0.9205\n",
            "Epoch 6/10\n",
            "149/149 [==============================] - 42s 285ms/step - loss: 0.2569 - accuracy: 0.9402\n",
            "Epoch 7/10\n",
            "149/149 [==============================] - 43s 285ms/step - loss: 0.2082 - accuracy: 0.9520\n",
            "Epoch 8/10\n",
            "149/149 [==============================] - 43s 286ms/step - loss: 0.1794 - accuracy: 0.9586\n",
            "Epoch 9/10\n",
            "149/149 [==============================] - 43s 290ms/step - loss: 0.1586 - accuracy: 0.9622\n",
            "Epoch 10/10\n",
            "149/149 [==============================] - 42s 285ms/step - loss: 0.1442 - accuracy: 0.9647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = './gpt2_model'\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "output_model_file = os.path.join(save_path, WEIGHTS_NAME)\n",
        "output_config_file = os.path.join(save_path, CONFIG_NAME)\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "model_to_save.config.to_json_file(output_config_file)\n",
        "\n",
        "# save tokenizer\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EHsXkw7ItH7",
        "outputId": "054d1095-7655-4659-dd75-fe45ba62ab2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gpt2_model/tokenizer_config.json',\n",
              " './gpt2_model/special_tokens_map.json',\n",
              " './gpt2_model/vocab.json',\n",
              " './gpt2_model/merges.txt',\n",
              " './gpt2_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prediction"
      ],
      "metadata": {
        "id": "6UKoaERiIupU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(path_model)\n",
        "model = TFGPT2LMHeadModel.from_pretrained(path_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXsJzRYdIxeb",
        "outputId": "275dbd07-f9d7-48df-fcca-acfdabfd999f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/TFM_Transformers/GPT2_CustomTokenizer_Model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"tacacs-server login host 10.15.1.10 vrf\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='tf')\n"
      ],
      "metadata": {
        "id": "1yHGoRnmLwAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(input_ids=input_ids, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=True, top_k=10, top_p=0.99, num_return_sequences=5)\n",
        "print(text)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN1POUuqRX3k",
        "outputId": "ae6abe8a-dc6c-4f1f-ce71-ab189ccfe01e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tacacs-server login host 10.15.1.10 vrf\n",
            "tacacs-server login host 10.15.1.10 vrf management seq-num 1 key 7 0x960c890aa72610d6aca0 port 1623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFpe6WELRkfO",
        "outputId": "6557a404-38cf-4caa-cbe7-f335c4c33061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tacacs-server login host 10.15.1.10 vrf management seq-num 1 key 7 0x960c890aa72610d6aca0 port 1623',\n",
              " 'tacacs-server login host 10.15.1.10 vrf management seq-num 1 key 7 0x960c890aa72610d6aca0 port 1623',\n",
              " 'tacacs-server login host 10.15.1.10 vrf management seq-num 1 key 7 0x960c890aa72610d6aca0 port 1623',\n",
              " 'tacacs-server login host 10.15.1.10 vrf management seq-num 1 key 7 0x960c890aa72610d6aca0 port 1623',\n",
              " 'tacacs-server login host 10.15.1.10 vrf management seq-num 1 key 7 0x960c890aa72610d6aca0 port 1623']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#!zip -r /content/file.zip ./gpt2_model\n",
        "#files.download(\"/content/file.zip\")"
      ],
      "metadata": {
        "id": "69xBzEnONXr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test and validation"
      ],
      "metadata": {
        "id": "gklEl6-dvBGe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Uy4nGP4sN9u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def find_config_start(config_lines, start_sequence):\n",
        "  i=0\n",
        "  config_found = np.False_\n",
        "  while not config_found:\n",
        "    if start_sequence in config_lines[i]:\n",
        "      config_found = True\n",
        "      config_index = i+1\n",
        "    else:\n",
        "      i+=1\n",
        "  return config_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dORduKuGsTrI"
      },
      "outputs": [],
      "source": [
        "configuration_start_point=\"!\"\n",
        "def get_config_lines(config_file):\n",
        "  file_handle = open(config_file, 'r')\n",
        "  config_lines = file_handle.readlines()\n",
        "  config_index = find_config_start(config_lines,configuration_start_point)\n",
        "  return config_lines[config_index:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAIenTLCsYnW"
      },
      "outputs": [],
      "source": [
        "def build_corpus(train_configs,path):\n",
        "  for config_file in train_configs:\n",
        "    corpus = \"\"\n",
        "    print('Config:',config_file)\n",
        "    config_lines = get_config_lines(join(path,config_file))\n",
        "    for i in range(len(config_lines)):\n",
        "      corpus += config_lines[i]\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_file = 'config_PE_errors.cfg'\n",
        "text=build_corpus([config_file],path_files)\n",
        "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
        "context_tokens=20\n",
        "\n",
        "errors=0\n",
        "for i in range(len(input_ids[0])-context_tokens):\n",
        "  outputs = model.generate(input_ids=input_ids[:,i:i+context_tokens+1],pad_token_id=tokenizer.eos_token_id, max_new_tokens=1, do_sample=True, top_k=50, top_p=0.99, num_return_sequences=6)\n",
        "  eq=0\n",
        "  for output in outputs:\n",
        "    if input_ids[:,i+context_tokens+1] == output[-1]:\n",
        "      eq=1\n",
        "  if eq==0:\n",
        "    errors+=1\n",
        "    print(f\"NOK: {errors} errors found\")\n",
        "    print(f\"Input: {tokenizer.decode(input_ids[0,i:i+context_tokens+2])}\")\n",
        "    print(f\"Output: {tokenizer.decode(outputs[0])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zBVjwoiBv1Ep",
        "outputId": "52ddcfcb-16de-4d65-ceea-217bc28620a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config: config_PE_errors.cfg\n",
            "NOK: 1 errors found\n",
            "Input: 0\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "! Software version: IOS-XR \n",
            "Output: 0\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "! Software version: IOS-XR 7\n",
            "NOK: 2 errors found\n",
            "Input: \n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "! Software version: IOS-XR 6\n",
            "Output: \n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "! Software version: IOS-XR \n",
            "\n",
            "NOK: 3 errors found\n",
            "Input: !\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "! Software version: IOS-XR 6.\n",
            "Output: !\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "! Software version: IOS-XR 6R\n",
            "NOK: 4 errors found\n",
            "Input: \n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "! Software version: IOS-XR 6.7\n",
            "Output: \n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "! Software version: IOS-XR 6.6\n",
            "NOK: 5 errors found\n",
            "Input: 2-profile-three\n",
            "!\n",
            "qos enable\n",
            "qos statistics\n",
            "!\n",
            "hostname PE-100\n",
            "Output: 2-profile-three\n",
            "!\n",
            "qos enable\n",
            "qos statistics\n",
            "!\n",
            "hostname PE-92\n",
            "NOK: 6 errors found\n",
            "Input: :PE-100-et-1-1-1 C-1 et-1-1-100\n",
            "Output: :PE-100-et-1-1-1 C-1 et-1-1-47\n",
            "NOK: 7 errors found\n",
            "Input: -1 et-1-1-100\n",
            " load-interval 30\n",
            " ip address 11.1.100\n",
            "Output: -1 et-1-1-100\n",
            " load-interval 30\n",
            " ip address 11.1.41\n",
            "NOK: 8 errors found\n",
            "Input: -100\n",
            " load-interval 30\n",
            " ip address 11.1.100.2/30\n",
            " mtu 1500\n",
            "Output: -100\n",
            " load-interval 30\n",
            " ip address 11.1.100.2/30\n",
            " mtu 9000\n",
            "NOK: 9 errors found\n",
            "Input:  10000\n",
            "!\n",
            "interface et-1-1-2\n",
            " description core:400G:PE-100\n",
            "Output:  10000\n",
            "!\n",
            "interface et-1-1-2\n",
            " description core:400G:PE-96\n",
            "NOK: 10 errors found\n",
            "Input: :PE-100-et-1-1-2 C-2 et-1-1-100\n",
            "Output: :PE-100-et-1-1-2 C-2 et-1-1-13\n",
            "NOK: 11 errors found\n",
            "Input: -2 et-1-1-100\n",
            " load-interval 30\n",
            " ip address 11.2.100\n",
            "Output: -2 et-1-1-100\n",
            " load-interval 30\n",
            " ip address 11.2.53\n",
            "NOK: 12 errors found\n",
            "Input:  ip ospf network point-to-point\n",
            " ip ospf cost 10000\n",
            "!\n",
            "interface et-1-1\n",
            "Output:  ip ospf network point-to-point\n",
            " ip ospf cost 10000\n",
            "!\n",
            "interface et-1-2\n",
            "NOK: 13 errors found\n",
            "Input:  10000\n",
            "!\n",
            "interface et-1-1-3\n",
            " description core:400G:PE-100\n",
            "Output:  10000\n",
            "!\n",
            "interface et-1-1-3\n",
            " description core:400G:PE-85\n",
            "NOK: 14 errors found\n",
            "Input: :PE-100-et-1-1-3 C-3 et-1-1-100\n",
            "Output: :PE-100-et-1-1-3 C-3 et-1-1-92\n",
            "NOK: 15 errors found\n",
            "Input: -3 et-1-1-100\n",
            " load-interval 30\n",
            " ip address 11.3.100\n",
            "Output: -3 et-1-1-100\n",
            " load-interval 30\n",
            " ip address 11.3.30\n",
            "NOK: 16 errors found\n",
            "Input:  10000\n",
            "!\n",
            "interface et-1-1-4\n",
            " description core:400G:PE-100\n",
            "Output:  10000\n",
            "!\n",
            "interface et-1-1-4\n",
            " description core:400G:PE-20\n",
            "NOK: 17 errors found\n",
            "Input: :PE-100-et-1-1-4 C-4 et-1-1-100\n",
            "Output: :PE-100-et-1-1-4 C-4 et-1-1-80\n",
            "NOK: 18 errors found\n",
            "Input: -4 et-1-1-100\n",
            " load-interval 30\n",
            " ip address 11.4.100\n",
            "Output: -4 et-1-1-100\n",
            " load-interval 30\n",
            " ip address 11.4.2\n",
            "NOK: 19 errors found\n",
            "Input: -100\n",
            " load-interval 30\n",
            " ip address 11.4.100.2/30\n",
            " mtu 1500\n",
            "Output: -100\n",
            " load-interval 30\n",
            " ip address 11.4.100.2/30\n",
            " mtu 9000\n",
            "NOK: 20 errors found\n",
            "Input:  min-links 2\n",
            " ip ospf network point-to-point\n",
            " ip ospf cost 10000\n",
            "!\n",
            "\n",
            "\n",
            "Output:  min-links 2\n",
            " ip ospf network point-to-point\n",
            " ip ospf cost 10000\n",
            "!\n",
            "interface\n",
            "NOK: 21 errors found\n",
            "Input: \n",
            "!\n",
            "\n",
            "interface et-1-2-1\n",
            " description edge:10G:PE-100\n",
            "Output: \n",
            "!\n",
            "\n",
            "interface et-1-2-1\n",
            " description edge:10G:PE-19\n",
            "NOK: 22 errors found\n",
            "Input: 1\n",
            " description edge:10G:PE-100-et-1-2-1 E-100\n",
            "Output: 1\n",
            " description edge:10G:PE-100-et-1-2-1 E-80\n",
            "NOK: 23 errors found\n",
            "Input: -100-1 et-1-1-1\n",
            " load-interval 30\n",
            " ip address 12.100\n",
            "Output: -100-1 et-1-1-1\n",
            " load-interval 30\n",
            " ip address 12.92\n",
            "NOK: 24 errors found\n",
            "Input:  1000\n",
            "!\n",
            "interface et-1-2-2\n",
            " description edge:10G:PE-100\n",
            "Output:  1000\n",
            "!\n",
            "interface et-1-2-2\n",
            " description edge:10G:PE-88\n",
            "NOK: 25 errors found\n",
            "Input: 2\n",
            " description edge:10G:PE-100-et-1-2-2 E-100\n",
            "Output: 2\n",
            " description edge:10G:PE-100-et-1-2-2 E-95\n",
            "NOK: 26 errors found\n",
            "Input: -100-2 et-1-1-1\n",
            " load-interval 30\n",
            " ip address 12.100\n",
            "Output: -100-2 et-1-1-1\n",
            " load-interval 30\n",
            " ip address 12.32\n",
            "NOK: 27 errors found\n",
            "Input: -2 et-1-1-1\n",
            " load-interval 30\n",
            " ip address 12.100.2\n",
            "Output: -2 et-1-1-1\n",
            " load-interval 30\n",
            " ip address 12.100.1\n",
            "NOK: 28 errors found\n",
            "Input: \n",
            " port-channel min-links 2\n",
            " ip ospf network point-to-point\n",
            " ip ospf cost 1000\n",
            "Output: \n",
            " port-channel min-links 2\n",
            " ip ospf network point-to-point\n",
            " ip ospf cost 10000\n",
            "NOK: 29 errors found\n",
            "Input: \n",
            " ip ospf network point-to-point\n",
            " ip ospf cost 1000\n",
            "!\n",
            "\n",
            "\n",
            "interface eth3\n",
            "Output: \n",
            " ip ospf network point-to-point\n",
            " ip ospf cost 1000\n",
            "!\n",
            "\n",
            "\n",
            "interface eth0\n",
            "NOK: 30 errors found\n",
            "Input: !\n",
            "\n",
            "\n",
            "interface eth3\n",
            " ip vrf forwarding management\n",
            " ip address 10.2.2.100\n",
            "Output: !\n",
            "\n",
            "\n",
            "interface eth3\n",
            " ip vrf forwarding management\n",
            " ip address 10.2.2.46\n",
            "NOK: 31 errors found\n",
            "Input: \n",
            " ip address 127.0.0.1/8\n",
            " ip address 10.2.2.100\n",
            "Output: \n",
            " ip address 127.0.0.1/8\n",
            " ip address 10.2.2.20\n",
            "NOK: 32 errors found\n",
            "Input: \n",
            " mtu 1500\n",
            "!\n",
            "router ospf\n",
            " ospf router-id 10.2.2.100\n",
            "Output: \n",
            " mtu 1500\n",
            "!\n",
            "router ospf\n",
            " ospf router-id 10.2.2.89\n",
            "NOK: 33 errors found\n",
            "Input: 10/32 area 0.0.0.0\n",
            " network 10.2.1.0/31\n",
            "Output: 10/32 area 0.0.0.0\n",
            " network 10.2.1.0/30\n",
            "NOK: 34 errors found\n",
            "Input: 2.3.0/30 area 0.0.0.0\n",
            " network 10.2.3\n",
            "Output: 2.3.0/30 area 0.0.0.0\n",
            " network 10.2.4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-541b7b941954>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0meq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcontext_tokens\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0meq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0meq\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5886\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5887\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5888\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 1784 of dimension 1 out of bounds. [Op:StridedSlice] name: strided_slice/"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-* Esto es para intentar sacar la salida del modelo en crudo como hacia Javier,\n",
        "parece que funciona!!! *-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UNS4txUjgaEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esto es para intentar sacar la salida del modelo en crudo como hacia Javier, parece que funciona!!!\n",
        "\n",
        "import numpy as np\n",
        "# Example text generation\n",
        "prompt = \"Software Baseline\"m\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"tf\")\n",
        "\n",
        "# Generate text manually\n",
        "max_length = 5\n",
        "temperature = 1.0  # Adjust temperature for randomness (higher values for more randomness, lower values for more determinism)\n",
        "generated_text = input_ids\n",
        "\n",
        "for _ in range(max_length):\n",
        "    logits = model(input_ids).logits[:, -1, :] / temperature\n",
        "    predicted_token = tf.random.categorical(logits, num_samples=1, dtype=tf.int32)[:, 0]\n",
        "    print(tokenizer.decode(predicted_token, skip_special_tokens=True))\n",
        "    input_ids = tf.concat([input_ids, tf.expand_dims(predicted_token, axis=1)], axis=1)\n",
        "    generated_text = tf.concat([generated_text, tf.expand_dims(predicted_token, axis=1)], axis=1)\n",
        "\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(generated_text.numpy().tolist()[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGFzuLH5diIl",
        "outputId": "a14c123f-b545-463c-a095-7ad8aaf72155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Version\n",
            ":\n",
            " 2\n",
            ".\n",
            "3\n",
            "Software Baseline Version: 2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "df = pd.read_json(join(path_dataset,\"dataset-configs-cisco.jsonl\"),orient=\"records\",lines=True)\n",
        "dataset_full = Dataset.from_pandas(df)\n",
        "\n",
        "total_words = tokenizer.vocab_size\n",
        "total_next_words = []\n",
        "for w in range(total_words):\n",
        "  total_next_words.append([])\n",
        "total_nexts = [0 for i in range(total_words)]\n",
        "for config in tqdm(dataset_full):\n",
        "  corpus = tokenizer.encode(config[\"config\"], return_tensors='tf')[0].numpy()\n",
        "  for i in range(len(corpus)-1):\n",
        "    current_token = corpus[i]\n",
        "    next_token = corpus[i+1]\n",
        "    current_token_nexts = total_next_words[current_token]\n",
        "    if next_token not in current_token_nexts:\n",
        "      total_next_words[current_token].append(next_token)\n",
        "      total_nexts[current_token]+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7Ncvg2CJJDp",
        "outputId": "67eedac8-2d48-48fa-c276-dd8d47bf7732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:02<00:00, 35.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for number,words in zip(total_nexts,total_next_words):\n",
        "  if number > 3:\n",
        "    word_list_decoded = []\n",
        "    for element in words:\n",
        "      word_list_decoded.append(tokenizer.decode([element], skip_special_tokens=True))\n",
        "    print(f\"{repr(tokenizer.decode([i], skip_special_tokens=True))}: {number} ---> {word_list_decoded}\")\n",
        "  i+=1\n",
        "print(len(total_nexts))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0iV3Mo1LZxp",
        "outputId": "9b849dc1-ecb7-436b-f44a-948eff50a43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'-': 137 ---> ['XR', '7', '6', 'R', '3', 'accton', 'r', 'encryption', 'dialog', 'balance', 'flow', 'ipv', 'port', 'id', 'hdr', 'l', 'ip', 'profile', 'three', '1', 'lookup', 'name', 'server', 'flap', 'bpdu', 'guard', 'setting', 'flaps', 'num', 'timeout', 'operator', 'admin', 'et', 'interval', 'channel', 'links', 'to', 'point', '2', '4', 'interfaces', 'interface', 'incompatible', '5', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100']\n",
            "'.': 104 ---> ['6', ' All', '3', '5', 'ai', '8', '9', '15', '1', '10', '11', '12', '13', '14', '2', '4', '0', 'management', '7', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100']\n",
            "'/': 5 ---> ['30', '8', '128', '32', '0']\n",
            "'0': 6 ---> ['\\n', ' port', ' vrf', '.', '/', ' 10']\n",
            "'1': 13 ---> ['\\n', ' macro', ' ipv', ' vxlan', '.', ' included', 'hde', '-', ' C', ' et', ' E', '/', ' secondary']\n",
            "'2': 9 ---> ['mrib', '-', '/', '\\n', ' C', ' et', '.', ' E', ' secondary']\n",
            "'3': 7 ---> ['\\n', '.', ' dest', ' C', ' et', '-', ' secondary']\n",
            "'4': 9 ---> ['\\n', ' dest', ' src', ' destl', '-', ' C', ' et', '.', ' secondary']\n",
            "'5': 5 ---> ['\\n', 'eyw', '-', '.', ' secondary']\n",
            "'6': 11 ---> ['R', '-', ' 5', ' dest', ' src', ' destl', 'aca', ' address', '\\n', '.', ' secondary']\n",
            "'7': 4 ---> ['-', '\\n', '.', ' secondary']\n",
            "'8': 4 ---> ['.', '\\n', '-', ' secondary']\n",
            "'9': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "':': 20 ---> [' Software', ' IOS', ' Copyright', ' 7', ' Hardware', ' ASR', ' SP', ' System', ' S', ' Package', ' P', ' 2', ' Installation', '\\n', ' Image', ' ONIE', ' x', '400', 'PE', '10']\n",
            "'o': 5 ---> [' logging', ' ip', ' errdisable', ' feature', ' username']\n",
            "'\\n': 31 ---> ['!', 'lo', 'serv', 'en', 'n', 'b', 'ip', 'f', 'qos', 'h', 'er', 't', 'aa', 's', 'u', '\\n', 'interface', ' description', ' load', ' ip', ' mtu', ' port', ' ipv', 'ro', ' ospf', ' bfd', ' timers', ' passive', ' network', 'l', ' exec']\n",
            "'ip': 6 ---> [' vrf', ' src', ' destl', ' domain', ' name', ' route']\n",
            "' ip': 4 ---> [' domain', ' address', ' ospf', ' vrf']\n",
            "' 5': 4 ---> ['\\n', ' ', ' time', ' facility']\n",
            "' level': 25 ---> [' cml', ' nsm', ' rip', ' ripng', ' ospf', ' isis', ' hostp', ' mrib', ' pim', ' auth', ' mstp', ' onm', ' hsl', ' oam', ' vlog', ' trill', ' vrrp', ' ndd', ' rib', ' bgp', ' l', ' lag', ' sflow', ' pserv', ' cmm']\n",
            "' management': 7 ---> ['\\n', ' mlfornetworkengineers', ' 8', ' seq', ' group', ' ', ' 0']\n",
            "' ospf': 7 ---> [' 5', '6', ' network', ' cost', '\\n', ' router', ' point']\n",
            "'30': 5 ---> ['\\n', ' area', '-', '.', ' secondary']\n",
            "' 0': 5 ---> ['x', ' 1', '.', '\\n', ' 39']\n",
            "' 2': 4 ---> ['.', ' key', 'c', '\\n']\n",
            "' address': 5 ---> [' 11', ' 12', ' 10', ' 127', ' ::']\n",
            "'server': 6 ---> [' vrf', ' login', ' enable', ' view', ' community', ' host']\n",
            "'port': 4 ---> [' srcl', ' protocol', ' next', ' 162']\n",
            "'10': 8 ---> [' vrf', 'G', '/', ' eth', '\\n', '-', '.', ' secondary']\n",
            "'15': 5 ---> ['.', ' 5', '\\n', '-', ' secondary']\n",
            "'eature': 4 ---> [' ssh', ' tacacs', ' ntp', ' rsyslog']\n",
            "' Software': 4 ---> [' version', ' Product', ' Feature', ' Baseline']\n",
            "'62': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'12': 5 ---> [' traps', '\\n', '-', '.', ' secondary']\n",
            "'60': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'64': 5 ---> ['-', 'x', '\\n', '.', ' secondary']\n",
            "'26': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'40': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'89': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'11': 5 ---> [' vrf', '\\n', '-', '.', ' secondary']\n",
            "'13': 5 ---> [' traps', '\\n', '-', '.', ' secondary']\n",
            "'14': 5 ---> [' vrf', '\\n', '-', '.', ' secondary']\n",
            "'20': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'32': 5 ---> [' area', '\\n', '-', '.', ' secondary']\n",
            "'36': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'77': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'86': 5 ---> ['_', '\\n', '-', '.', ' secondary']\n",
            "'16': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'17': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'18': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'19': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'100': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'21': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'22': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'23': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'24': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'25': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'27': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'28': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'29': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'31': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'33': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'34': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'35': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'37': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'38': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'39': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'41': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'42': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'43': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'44': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'45': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'46': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'47': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'48': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'49': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'50': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'51': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'52': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'53': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'54': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'55': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'56': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'57': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'58': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'59': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'61': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'63': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'65': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'66': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'67': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'68': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'69': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'70': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'71': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'72': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'73': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'74': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'75': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'76': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'78': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'79': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'80': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'81': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'82': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'83': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'84': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'85': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'87': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'88': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'90': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'91': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'92': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'93': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'94': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'95': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'96': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'97': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'98': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "'99': 4 ---> ['\\n', '-', '.', ' secondary']\n",
            "789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode([134], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLrqWhymW_sG",
        "outputId": "88341eb5-13ab-4149-881e-9b373c34db3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " port\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(join(path_dataset,\"dataset-configs-cisco.jsonl\"),orient=\"records\",lines=True)\n",
        "dataset_full = Dataset.from_pandas(df)\n",
        "len(dataset_full[0][\"config\"])\n",
        "\n",
        "corpus = tokenizer.encode(dataset_full[0][\"config\"], return_tensors='tf')\n",
        "print(corpus[0].numpy()[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOln66UKJ3TT",
        "outputId": "17491d13-1b67-4f21-97d9-79cba94e958b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "154\n"
          ]
        }
      ]
    }
  ]
}